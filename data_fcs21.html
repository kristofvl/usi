<!doctype html>
<html>
	<head>
		<link rel="stylesheet" href="usi.css" />
		<link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
		<meta
			name="viewport"
			content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"
		/>
	</head>
	<body>
		<div id="hdr"></div>
		<script async src="hdr.js" strng="Breathing In-Depth"></script>

		<div id="usi_page">
			<h3>Breathing In-Depth<br />Jochen Kempfle and Kristof Van Laerhoven</h3>

			<h3>Description</h3>
			<img
				align="right"
				width="512px"
				src="img/d/breathingindepth.jpg"
				style="margin-left: 12px; margin-bottom: 4px"
			/>
			<p>
				As depth cameras have gotten smaller, more affordable, and more precise,
				they have also emerged as a promising sensor in ubiquitous systems,
				particularly for detecting objects, scenes, and persons. This article
				sets out to systematically evaluate how suitable depth data can be for
				picking up users' respiration, from small distance changes across the
				torso over time. We contribute a large public dataset of depth data over
				time from 19 persons taken in a large variety of circumstances. On this
				data, we evaluate and compare different state-of-the-art methods and
				show that their individual performance significantly depends on a range
				of conditions and parameters. We investigate the influence of the
				observed torso region (e.g. the chest), the user posture and activity,
				the distance to the depth camera, the respiratory rate, the gender, and
				user specific peculiarities. Best results hereby are obtained from the
				chest whereas the abdomen is least suited for detecting the user's
				breathing. In terms of accuracy and signal quality, the largest
				differences are observed on different user postures and activities. All
				methods can maintain a mean accuracy of above 92% when users are
				sitting, but half of the observed methods only achieve a mean accuracy
				of 51% while standing. When users are standing and additionally move
				their arms in front of their upper body, mean accuracy values between
				the worst and best performing methods range from 21% to 87%. Increasing
				the distance to the depth camera furthermore results in lower signal
				quality and decreased accuracy on all methods. Optimal results can be
				obtained at distances of 1 to 2 metres. Different users have been found
				to deliver varying qualities of breathing signals. Causes range from
				clothing, over long hair, to movement. Other parameters have shown to
				play a minor role in the detection of users' breathing.
			</p>

			<h3>Download</h3>
			<p>
				The original data as used in the paper can be downloaded via this link:
				<a href="https://uni-siegen.sciebo.de/s/jJxrshcoTsqw9nR"
					>FCS'21 dataset (21.4 GB)</a
				>
			</p>

			<h3>Citation</h3>
			<p>
				<a href="http://dx.doi.org/10.3389/fcomp.2021.757277"
					>Breathing In-Depth: A Parametrization Study on RGB-D Respiration
					Extraction Methods. Jochen Kempfle and Kristof Van Laerhoven.
					Frontiers 2021.</a
				>
			</p>

			<h3>Disclaimer</h3>
			<p>
				You may use this data for scientific, non-commercial purposes, provided
				that you give credit to the owners when publishing any work based on
				this data.
			</p>
		</div>
		<div id="usi_ftr"></div>
	</body>
</html>
